{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化神经网络的手段\n",
    "1. 激活函数的优化\n",
    "2. 学习率采用指数衰减学习率\n",
    "3. 参数的更新使用滑动平均值\n",
    "4. 防止过拟合加入正则化项"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 激活函数\n",
    "\n",
    "根据实际情况选择合适的激活函数， 加入激活函数是为了解决线性模型不能解决异或问题，因为引入了非线性，提高了模型的表达力，使模型具有更好的区分度；<br>损失函数(loss function) : 预测值(y) 与已知答案(y_) 的差距； NN优化目标：loss最小；<br>\n",
    "主流的loss 计算有三种：均方误差MSE、交叉熵CE(cross entropy)、自定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MSE 激活函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mse = reduce_mean(tf.square(y - y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉熵激活函数\n",
    "交叉熵ce(cross entropy): 表征两个概率分布之间的距离，交叉熵越大，两个概率分布越远， 交叉熵越小，两个概率分布越近<br>\n",
    "后面的参数 1e-12 和 1.0 都是对y做限制，为了使输入的y有意义， 当y小于1e-12时，就让y=1e-12, 当y大于1.0的时候，就让y=1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = - tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y, 1e-12, 1.0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在实际操作中，为了让前向传播计算出的结果满足概率分布，也就是让推测出的n分类的n个输出(y1, y2, …, yn), \n",
    "每一个都在[0, 1] 之间，并且这n个输出的和为1. \n",
    "通过softmax()函数实现, y1, y2 … yn 这n个数通过softmax函数处理后，得到的结果是符合概率分布的。\n",
    "将输入y1, y2, … yn 通过softmax函数后，再与标准答案y,求交叉熵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "cem = tf.reduce_mean(ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习率\n",
    "\n",
    "学习率：就是每次参数更新的幅度；参数的更新，向着损失函数梯度下降的方向； 目的是找到某个w, 使得损失函数的梯度最小，也就是loss的导数为0；优化参数的目关于学习率设置的问题：\n",
    "<br>\n",
    "关于学习率设置需要合适：如果学习率过大，会导致震荡而不收敛; 如果学习率过小，会导致收敛速度极慢\n",
    "<br>\n",
    "指数衰减的学习率：先下降的速度比较快，之后缓慢的下降<br>\n",
    "学习率基数(初始值)* 学习率衰减率(0~1)^ 多少轮更新一次学习率 = 总样本数 / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LEARNING_RATE_BASE * LEARNING_RATE_DECAY^(global_step / LEARNING_RATE_STEP)\n",
    "\n",
    "# 计数器，记录当前共运行了多少轮，只用于计数，所以我们将属性标注为不可训练\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP, LEARNING_RATE_DECAY, staircase=True)\n",
    "\n",
    "\"\"\"\n",
    "LEARNING_RATE_BASE: 学习率的基数，最开始设置的学习率\n",
    "LEARNING_RATE_DECAY: 学习率衰减率\n",
    "staircase=True: 表示呈阶梯曲线衰减\n",
    "staircase=True: 表示学习率是平滑下降的曲线\n",
    "\"\"\"\n",
    "\n",
    "# 设置损失函数 loss=(w+1)^2, 令w初值是常数10， 反向传播就是求最优值w,即求最小loss对应的w值\n",
    "# 使用指数衰减的学习率，在迭代初期得到较高的下降速度，可以在较小的训练轮数下取得更有收敛度\n",
    "\n",
    "LEARNING_RATE_BASE = 0.1 # 最初学习率\n",
    "LEARNING_RATE_DECAY = 0.99 # 学习率衰减\n",
    "LEARNING_RATE_STEP = 1  # 喂入多少轮batch_size后，更新一次学习率，一般设置为: 总样本数 / batch_size\n",
    "\n",
    "# 运行了几轮BATCH_SIZE的计数器，初始值为0， 设为不被训练\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "# 定义指数下降学习率,在梯度下降中，学习率不再是固定的值，而是动态调整的\n",
    "learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, global_step, LEARNING_RATE_STEP, LEARNING_RATE_DECAY, staircase=True)\n",
    "\n",
    "# 定义待优化参数，初始值为5\n",
    "w = tf.Variable(tf.constant(5, dytype=tf.float32))\n",
    "\n",
    "# 定义损失函数loss\n",
    "loss = tf.square(w+1)\n",
    "\n",
    "# 定义反向传播方法,这里的学习率learning_rate不再是一个定值了，而是动态的变化的；\n",
    "# 有一点不太明白的是 global_step=global_step 是什么意思？\n",
    "# global_step refers to the number of batches seen by the graph. \n",
    "# Every time a batch is provided, the weights are updated in the direction that minimizes the loss. \n",
    "# global_step just keeps track of the number of batches seen so far. \n",
    "# When it is passed in the minimize() argument list, the variable is increased by one.\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "# 生成会话，训练40轮\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "    for i in range(40):\n",
    "        sess.run(train_step)\n",
    "        # 保存学习率的值，用于打印输出\n",
    "        learning_rate_val = sess.run(learning_rate)\n",
    "        global_step_val = sess.run(global_step)\n",
    "        w_val = sess.run(w)\n",
    "        loss_val = sess.run(loss)\n",
    "        print(\"After %s steps， global_step is %f, w is %f, learning rate is %f, loss is %f\" %(i, global_step_val, w_val, learning_rate_val, loss_val))\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "滑动平均 ema\n",
    "滑动平均(影子值)：记录了每一个参数一段时间内过往值的平均\n",
    "不仅表示了当前值，还表示了过去一段时间内的平均值，这样可以增加模型的泛化性\n",
    "针对所有参数：w 和 b 进行滑动平均\n",
    "滑动平均就像是给参数加了影子，参数变化，影子缓慢跟随\n",
    "\n",
    "影子初始值 = 参数初始值\n",
    "影子 = 衰减率 * 影子 + (1 - 衰减率) * 参数\n",
    "衰减率 = min{MOVING_AVERAGE_DECAY, (1+轮数)/(10+轮数)}\n",
    "\n",
    "MOVING_AVERAGE_DECAY 是一个超参数，一般赋一个比较大的值，比如0.99， 参数w1为0， 轮数global_step为0， w1的滑动平均值为0\n",
    "当参数w1被更新为1的时候： 得到w1的滑动平均值\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVING_AVERAGE_DECAY 滑动平均衰减率\n",
    "# global_step 当前轮数\n",
    "\n",
    "ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "\n",
    "# 定义了对括号里的参数求滑动平均\n",
    "ema_op = ema.apply([])\n",
    "\n",
    "# 自动将所有待训练的参数汇总成列表\n",
    "ema_op = ema.apply(tf.trainable_varibales())\n",
    "\n",
    "# 每运行此句，所有待优化的参数求滑动平均\n",
    "# 将计算互动平均和训练过程绑定在一起运行，合成一个训练节点\n",
    "with tf.control_dependencies([train_step, ema_op]):\n",
    "  train_op = tf.no_op(name=\"train\")\n",
    "\n",
    "# 查看某个参数的滑动平均值\n",
    "ema.avarage(参数名)\n",
    "\n",
    "\n",
    "#1 定义变量以及滑动平均类\n",
    "# 定义一个32位浮点变量，初始值为0.0, 这个代码就是不断更新w1参数，优化w1参数， 滑动平均做了个w1的影子\n",
    "w1 = tf.Variable(0, dtype=tf.float32)\n",
    "\n",
    "# 定义num_updates(NN的迭代次数), 初始值为0， 不可被优化(训练),这个参数不训练\n",
    "golbal_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "# 实例化滑动平均类，衰减率为0.99， 当前轮数：global_step\n",
    "MOVING_AVERAGE_DECAY = 0.99\n",
    "ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "\n",
    "# ema.apply后面的括号里是更新列表，每次运行sess.run(ema_op)时， 对更新列表中的元素求滑动平均\n",
    "# 在实际应用中会使用tf.trainable_variables()自动将所有待训练的参数汇总为列表\n",
    "# ema.apply([w1])\n",
    "ema_op = ema.apply(tf.trainable_variables())\n",
    "\n",
    "#2 查看不同迭代中变量取值的变化\n",
    "with tf.Session() as sess:\n",
    "    # 初始化\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    # 用ema.average(w1)获取w1滑动平均(要运行多个节点， 作为列表中的元素列出，写在sess.run中)\n",
    "    # 打印出当前参数w1和w1的滑动平均\n",
    "    print(sess.run([w1, ema.average(w1)]))\n",
    "  \n",
    "    # 参数w1的值赋值为1\n",
    "    sess.run(tf.assign(w1, 1))\n",
    "    sess.run(rma_op)\n",
    "    print(sess.run([w1, ema.average(w1)]))\n",
    "  \n",
    "    # 更新step和w1的值，模拟出100轮迭代后，参数w1变为10\n",
    "    sess.run(tf.assign(global_step, 100))\n",
    "    sess.run(tf.assign(w1, 10))\n",
    "    sess.run(rma_op)\n",
    "    print(sess.run([w1, ema.average(w1)]))\n",
    "  \n",
    "    # 每次sess.run会更新一次w1的滑动平均值\n",
    "    sess.run(ema_op)\n",
    "    print(sess.run([w1, ema.average(w1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "正则化 regularization\n",
    "包含了正则化的模型曲线会更加的平滑，数据集中的噪声将会更小\n",
    "正则化缓解过拟合\n",
    "正则化在损失函数中引入模型复杂度指标，利用给W加权值，弱化了训练数据的噪声(一般不正则化b)\n",
    "loss = loss(y, y_) + REGULARIZER \\* loss(w)\n",
    "loss(y, y_) : 模型中所有参数的损失函数，如：交叉熵、均方误差mse\n",
    "REGULARIZER : 用超参数REGULARIZER给出参数w在总loss中的比例，即正则化的权重\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1正则化,对参数的绝对值求和\n",
    "loss(w) = tf.contrib.layers.l1_regularizer(REGULARIZER)(w)\n",
    "\n",
    "# L2正则化,对参数的平方求和\n",
    "loss(w) = tf.contrib.layers.l2_regularizer(REGULARIZER)(w)\n",
    "\n",
    "# 把计算好了的所有w正则化加入到集合losses中做加法\n",
    "tf.add_to_collection(\"losses\", tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "# tf.add_n 可以将losses中的所有值相加； 交叉熵 + 正则项 = loss 函数\n",
    "loss = cem + tf.add_n(tf.get_collection(\"losses\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "seed = 2\n",
    "\n",
    "# 基于seed产生随机数\n",
    "rdm = np.random.RandomState(seed)\n",
    "\n",
    "# 随机数返回300行2列的矩阵中取出一行，判断如果两个坐标的平方和小雨2， 给y赋值1，其余赋值0，作为标签\n",
    "Y_ = [int(x0*x0 + x1*x1 < 2) for (x0, x1) in X ]\n",
    "\n",
    "# 遍历Y中的每个元素，1 赋值为'red', 其余赋值为‘'blue', 便于可视化显示\n",
    "Y_c = [['red' if y else 'blur'] for y in Y_]\n",
    "  \n",
    "# 对数据集X和标签Y进行形状整理，第一个元素为-1表示根据最后列计算，第二个元素表示多少列，可见X为2列，Y为1列\n",
    "# X 是n行两列，Y 是n行1列\n",
    "X = np.vstack(X).reshape(-1, 2)\n",
    "Y_ = np.vstack(Y_).reshape(-1, 1)\n",
    "\n",
    "# 用plt.scatter 画出离散点\n",
    "plt.scatter(X[:,0], X[:,1], c=np.squeeze(Y_c))\n",
    "plt.show()\n",
    "\n",
    "# 定义神经网络的输入、参数和输出，定义前向传播过程\n",
    "def get_weight(shape, regularizer):\n",
    "    w = tf.Variable(tf.random_normal(shape), dtype=tf.float32)\n",
    "    tf.add_to_collection('losses', tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "def get_bias(shape):\n",
    "    b = tf.Variable(tf.constant(0.01, shape=shape))\n",
    "    return b\n",
    "\n",
    "# 占位\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "# 输入层2个节点，隐藏层11个节点，输出层1个节点\n",
    "w1 = get_weight([2, 11], 0.01)\n",
    "# 偏置 b1是11个\n",
    "b1 = get_bias([11])\n",
    "y1 = tf.nn.relu(tf.matmul(x, w1) + b1)\t# 要过激活函数\n",
    "\n",
    "w2 = get_weight([11, 1], 0.01)\n",
    "b2 = get_bias([1])\n",
    "y = tf.matmul(y1, w2) + b2\t\t# 输出层不过激活\n",
    "\n",
    "# 定义损失函数为mean square error\n",
    "loss_mse = tf.reduce_mean(tf.square(y - y_))\n",
    "loss_total = loss_mse + tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "# 定义反向传播方法：如果不含正则化\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_mse)\n",
    "# 定义反向传播方法：如果包含正则化\n",
    "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss_total)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    # 训练40000次，每次喂入BATCH_SIZE个数据\n",
    "    STEPS = 40000\n",
    "    for i in range(STEPS):\n",
    "        start = (i*BATCH_SIZE) % 300\n",
    "        end = start + BATCH_SIZE\n",
    "        sess.run(train_step, feed_dict={x:X[start:end], y_:Y_[start:end]})\n",
    "    if i%2000 == 0:\n",
    "        loss_mse_v = sess.run(loss_mse, feed_dict={x:X, y_:Y_})\n",
    "        print(\"After %d steps, loss is: %f\" %(i, loss_v))\n",
    "  \n",
    "    # xx在-3到3之间以步长0.01为间距生成网格坐标，组成xx,yy坐标集，二维坐标点\n",
    "    xx, yy = np.mgrid[-3:3:0.01, -3:3:0.01]\n",
    "    \n",
    "    # 将xx,yy拉直，并合并成一个2列的矩阵，得到一个网格坐标点的集合\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # 将网格坐标点喂入神经网络，probs为输出，也就是神经网络推算出的结果\n",
    "    probs = sess.run(y, feed_dict={x:grid})\n",
    "    \n",
    "    # probs的shape调整成xx的样子\n",
    "    probs = probs.reshape(xx.shape)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=np.squeeze(Y_c))\n",
    "# 画等高线,为probs为0.5的点上色\n",
    "plt.contour(xx, yy, probs, levels=[.5])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
