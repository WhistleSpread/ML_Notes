{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "基于Tensorflow的NN: \n",
    "\n",
    "1. 用张量表示数据，\n",
    "2. 用计算图搭建神经网络\n",
    "3. 用会话执行计算图\n",
    "4. 优化线上的权重（参数）\n",
    "5. 得到模型\n",
    "\n",
    "\n",
    "深度学习的基本套路\n",
    "\n",
    "- 定义计算图，初始化计算图中的变量\n",
    "- 喂入数据，由计算图前向传播得到预测值\n",
    "-  比较预测值和真实值，使 **loss function**评价当前模型(计算图和计算图中的变 )\n",
    "- 使用优化器优化计算图中的变量，降低**loss function**\n",
    "- 重复第二步到**loss**收敛\n",
    "\n",
    "\n",
    "这段代码描述了一个计算过程，就是一张计算图，从print 输出结果可以看出，\n",
    "只是显示了结果是一个张量，并没有实际运算张量具体的值，所以，计算图只描述计算过程，不计算运算结果；\n",
    "\n",
    "计算图(Grapy): 搭建神经网络的计算过程，只搭建，不运算  它是承载多个这种计算节点的图，\n",
    "神经网络的基本单元是神经元，用张量运算来描述一个神经元：y = X*W = x1*w1 + x2*w2;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前向传播用随机数来生成初始的参数变量\n",
    "参数，也就是神经元上的参数w， 用变量表示，随机给出初值\n",
    "采用正态分布的方法随机生成2x3矩阵，其中正态分布的标准差为2, 均值为0，随机种子为1(如果去掉随机种子，每次生成的随机数将会是一样的;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = tf.Variable(tf.random_normal([2, 3], stddev=2, mean=0, seed=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "去掉过大偏离点的正态分布,生成出来的超过两个标准差将会重新生成, \n",
    "shape表示生成张量的维度，mean是均值，stddev是标准差。\n",
    "这个函数产生正太分布，均值和标准差自己设定。这是一个截断的产生正太分布的函数，\n",
    "就是说产生正太分布的值如果与均值的差值大于两倍的标准差，那就重新生成。和一般的正太分布的产生随机数据比起来，\n",
    "这个函数产生的随机数与均值的差距不会超过两倍的标准差，但是一般的别的函数是可能的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.truncated_normal(shape=[10,10], mean=0, stddev=1)\n",
    "\n",
    "# 平均分布, 返回4*4的矩阵，产生于low和high之间，产生的值是均匀分布的。\n",
    "tf.random_uniform((4, 4), minval=-0.5, maxval=0.5, dtype=tf.float32)\n",
    "\n",
    "# tf.zeros 全零数组\n",
    "tf.zeros([3, 2],tf.int32)\n",
    "# tf.ones 全1数组\n",
    "# tf.ones([3, 2],tf.int32) 生成[[1,1], [1, 1], [1, 1]]\n",
    "# tf.fill 全定值数组\n",
    "# tf.fill([3, 2], 6) 生成 [[6, 6], [6, 6], [6, 6]]\n",
    "# tf.constant 直接给值\n",
    "# tf.constant([3, 2, 1]) 生成 [3, 2, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "神经网络的实现过程：\n",
    "1. 准备数据集， 提取特征，作为输入数据喂给神经网络(Neural Network, NN)\n",
    "2. 搭建NN结构， 从输入到输出(先搭建计算图，再用会话执行)\n",
    "   （NN**前向传播**算法 ==> 计算输出）\n",
    "3. 大量特征数据喂给NN, 得到大量输出，把每一次的输出和标准答案的差反向传回给神经网络，调整神经网络的参数（迭代优化NN参数），直到模型达到要求。\n",
    "   （NN**反向传播**算法 ==> 优化参数训练模型）\n",
    "4. 使用训练好的模型预测和分类\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 前向传播实现过程：搭建模型的计算过程，让模型具有推理能力，可以针对一组输入，给出相应的输出\n",
    "- 定义输入和参数\n",
    "- 定义前向传播过程\n",
    "- 用会话计算结果(在上面那个不计算，要计算必须用session)\n",
    "- 把初始化所有变量的函数(tf.global_variables_initializer())简写为init_op\n",
    "- 初始化所有的变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('y is:', array([[3.0904665]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([[0.7, 0.5]])\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    print(\"y is:\", sess.run(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 两层简单神经网络(全连接)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('y is:', array([[3.0904665]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# 定义输入和参数\n",
    "# 用placeholder实现输入定义 (sess.run中喂一组数据)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(1, 2))\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "\n",
    "# 如果需要一次喂入n组特征，可是还不知道要喂入多少组，可以用 shape=(None, 2)\n",
    "# x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "# 这样在使用的时候，就可以一次喂入多组数据\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    # 初始化所有的变量\n",
    "    sess.run(init_op)\n",
    "    ########### 这个地方有点问题，关于一次性喂入多个数据的处理\n",
    "#     print(\"y is:\", sess.run(y, feed_dict = {x:[[0.7, 0.5],[0.3, 0.4],[0.4, 0.5]]}))\n",
    "    print(\"y is:\", sess.run(y, feed_dict = {x:[[0.7, 0.5]]}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反向传播的目的是为了优化模型参数。<br>\n",
    "反向传播 ==> 训练模型参数， 在所有参数上用梯度下降，使NN模型在训练数据上的损失函数最小<br>\n",
    "损失函数(loss function):预测值(y)与已知答案(y*)(标签)的差距<br>\n",
    "均方误差 Mean Square Error  MSE(y*，y)<br>\n",
    "定义损失函数<br>\n",
    "loss = tf.reduce_mean(tf.square(y* - y))<br>\n",
    "\n",
    "反向传播训练方法：以减小loss值为优化目标<br>\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)<br>\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate, momentum).minimize(loss)<br>\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)<br>\n",
    "在这三种优化器中，都需要一个叫做**学习率**的参数，学习率用来决定参数每次更新的幅度，<br>\n",
    "在使用的时候，我们一般先选一个比较小的参数，比如0.001<br> \n",
    "\n",
    "\n",
    "神经网络搭建八股：准备、前传、反传、迭代<br>\n",
    "准备：import 、常量定义、生成数据集<br>\n",
    "前向传播：定义输入，参数，输出<br>\n",
    "反向传播：定义损失函数(loss)、反向传播方法(train_step)<br>\n",
    "生成会话session,训练Steps轮<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward.py\n",
    "\n",
    "def forward(x, regularizer):\n",
    "    \"\"\"\n",
    "    完成网络的设计，给出从输入到输出的数据通路\n",
    "    \"\"\"\n",
    "    w = \n",
    "    b = \n",
    "    y = \n",
    "    return y\n",
    "\n",
    "def get_weight(shape, regularizer):\n",
    "    \"\"\"\n",
    "    w的形状shape， 正则化权重\n",
    "    \"\"\"\n",
    "    w = tf.Variable()\t\t# 赋初值，括号中是赋初值的方法\n",
    "\n",
    "    # 将正则项加入到总的损失函数中\n",
    "    tf.add_to_collection(\"losses\", tf.contrib.layers.l2_regularizer(regularizer)(w))\n",
    "    return w\n",
    "\n",
    "def get_bias(shape):\n",
    "    \"\"\"\n",
    "    与参数b有关，函数的参数是b的形状，其实就是某层中b的个数\n",
    "    \"\"\"\n",
    "    b = tf.Variable()\n",
    "    return b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward.py\n",
    "\n",
    "def backward():\n",
    "    # 用placeholder 给输入x和y_标签占位\n",
    "    x = tf.placeholder(   )\n",
    "    y_ = tf.placeholder(   )\n",
    "\n",
    "    # 利用forward 复现前向传播的结构，计算得到y\n",
    "    y = forward.forward(x, REGULARIZER)\n",
    "\n",
    "    # 定义轮数计数器global_step\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # 定义损失函数loss, loss 可以选择mse、自定义、交叉熵, 表示通过前向过程计算的y与标准答案的差距\n",
    "    # 用均方误差 mean square error\n",
    "    loss_mse = tf.reduce_mean(tf.square(y-y_))\n",
    "    # 用交叉熵 cross entropy\n",
    "    ce = tf.nn.sparse_softmax_cross_entripy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    cem = tf.reduce_mean(ce)\n",
    "\n",
    "    # 加入正则化：lossfunction + 正则项 \n",
    "    loss = y与y_的差距 + tf.add_n(tf.get_collection(\"losses\"))\n",
    "\n",
    "    # 指数衰减学习率，动态计算学习率\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "    LEARNING_RATE_BASE, \n",
    "    global_step, \n",
    "    LEARNING_RATE_STEP, \n",
    "    LEARNING_RATE_DECAY, \n",
    "    staircase=True)\n",
    "\n",
    "    # train_step 定义学习率，学习步长\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss, global_step = global_step)\n",
    "\n",
    "    # 滑动平均\n",
    "    ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY, global_step)\n",
    "    ema_op = ema.apply(tf.trainable_variables())\n",
    "    \n",
    "    with tf.control_dependencies([train_step, ema_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "    # 所有变量初始化\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "\n",
    "    # 训练steps轮\n",
    "    for i in range(STEPS):\n",
    "        # 每轮调用sess.run,执行训练过程，传入学习率和学习的数据\n",
    "        sess.run(train_step, feed_dict={x: , y_: })\n",
    "        # 每运行一定轮数，打印出当前信息\n",
    "        if i % 轮数 == 0:\n",
    "            print \n",
    "\n",
    "if __name__ == '__main__':\n",
    "   backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
