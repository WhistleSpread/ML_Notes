因为二次代价函数在使用梯度下降法的时候，参数调整不太合理，所以，使用交叉熵来作为loss function 

不改变 activate function, 改变loss function 

loss function 不再是使用mean square error, 

如果输出神经元是线性的，那么二次代价函数就是一种合适的选择，如果输出神经元是S型函数，那么比较适合用交叉熵代价函数。一般来说，存在梯度消失的情况的loss function,我们都改为交叉熵代价函数

那么是不是说，线性的输出函数和二次代价函数搭配起来比较好，

如果输出的函数是S型的函数，比如说是sigmoid 函数，tanh函数，那么用交叉熵来作为代价函数就比较好。



交叉熵和二次代价函数最大的区别在于：交叉熵调整合理，训练模型比较快。模型收敛的速度比较快

最简单的手写数字识别，通过构建一个简单的神经网络(没有隐藏层，激活函数为softmax, 损失函数loss function 为二次函数 mean square error ) 得到的正确率为91%, 可是如果增加隐藏层，通过将激活函数用softmax, 损失函数用交叉熵的话，得到的正确率可以达到95%



对数似然函数(log-likelihood cost):

对数似然函数常常用来作为softmax回归的代价函数，如果输出层神经元是sigmoid，可以采用交叉熵代价函数，而深度学习中，更加普遍的做法是将softmax作为最后一层（用来将数值转化为概率），此时常用的代价函数是对数似然函数

对数似然函数与softmax的组合和交叉熵与sigmoid函数的组合非常相似，对数似然代价函数在二分类时可以化简为交叉熵代价函数的形式

如果是用的softmax，那么就使用对数似然函数作为loss function，如果激活函数使用的是sigmoid，或者是tanh, 那么loss function用交叉熵比较好



在tensorflow中的使用：

```python
tf.nn.sigmoid_cross_entropy_with_logits() 表示与sigmoid搭配的交叉熵
tf.nn.softmax_cross_entropy_with_logits() 表示与softmax搭配的交叉熵
```







用drop out 技术来改进过拟合现象

一般解决过拟合 的方法主要有：增加数据集、正则化方法、Dropout等

正则化：

在cost function 后面加上一个正则项，这个正则项其实是网络中的所有权值的平方的累加；由于目标函数是最小化loss function，所以在最小化的过程中会使得某些权值w会减小到0, 最后使某些权值越来越小了之后，就使得整个网络变得简单；因为如果连接某一个神经元的参数都等于0的话，那么我们就可以认为这个神经元是不存在的；

这个正则化项的引入就是为了尽可能的使参数小。就是为了减小网络的复杂程度





drop out 的工作原理就是在每次迭代的过程中，使得每次只有部分神经元是工作的，部分神经元是不工作的；在每次迭代中，都换一批神经元，让他们不工作，让中间层的部分神经元工作，部分神经元不工作；

最后测试的时候使用所有的神经元，在训练的时候只用部分的神经元进行训练；一般来讲，使用dropout之后，模型的收敛速度会变慢。



介绍两个业界有名的图像识别的神经网络

GoogleNet 以及 AlexNet







优化器的介绍

```python
tf.train.GradientDescentOptimizer
tf.train.AdadeltaOptimizer
tf.train.AdagradOptimizer
tf.train.AdagradDAOptimizer
tf.train.MomentumOptimizer
tf.train.AdamOptimizer
tf.train.FtrlOptimizer
tf.train.ProximalGradientDescentOptimizer
tf.train.ProximalAdagradOptimizer
tf.train.AdamOptimizer
tf.train.RMSPropOptimizer

```



标准梯度下降法，随机梯度下降法，批量梯度下降法

一般使用Adam用到的学习率都要比较小，0.01都算大的，但是Adam的收敛速度比随机梯度下降法慢；



tensorboard 是tensorflow中的一个可视化工具，可以非常方便的可视化训练的过程 

tensorboard的使用其实还是蛮简单的，只是需要自己定义namescope，然后自己给他们取名字。

至于查看神经网络运行时候的数据，





卷积神经网络

传统的神经网络的问题：

1. 权值太多，计算量太大
2. 权值太多，需要大量样本进行训练

用prior knowledge 将原来的fully connected layer里面的一些参数拿掉就变成cn

如果数据很小的话，那么设计网络的模型就不需要太复杂，如果模型太复杂，就很容易出现过拟合的现象，但是如果数据量非常大，可是模型太过简单，就很难达到比较好的拟合状态；经验(样本的大小最好是权值的5～30倍)。

局部感受野— 神经感知机neocognitron

CNN 卷积神经网络：通过感受野和权值共享减少了神经网络需要训练的参数个数

全连接的神经网络：fully connected neural network

卷积神经网络：locally connected neural network

局部感受野：在隐藏层的神经元只连接输入层的一部分；

权值共享：在隐藏层中，由于每个神经元只连接一部分输入层，所以共享，就像每一个隐藏层的神经元进行了全连接。

卷积的操作：图片，卷积核，卷积核的大小，用卷积核对大的图片进行采样(卷积操作)，每进行一次卷积操作就会得到一个值，采样完成后，会得到一个特征图(Convolved feature);

卷积核的定义有很多种，例如卷积核有不同的大小，卷积核中的数字也可以不同；这个卷积核可以认为是一个滤波器，一张图片，经过一个卷积核的卷积操作后，可以得到一个特征图；不同的卷积核得到的特征图不同，所以可以认为不同的卷积核用于对图片的不同特征进行采样；

如果我们有多个卷积核，那么我们就可以对一种图片从多个不同的角度来进行采样，抽取不同维度的特征图；单一的特征对于图片分类的效果不是很好；

在卷积神经网络中会有卷积层和池化层，池化也是类似卷积的操作，池化的类型：max-pooling: 划分为几个区域，然后求这些区域的最大值，mean-pooling: 划分为几个区域，然后求这些区域的平均值。random-pooling 在区域中既不是找最大，也不是最小，而是随机取一个值；一般卷积层之后都后将一个池化的操作；

两种padding的方法：same padding:给平面外部补0，卷积窗口采样后得到一个跟原来平面大小相同的平面；也就是得到的特征图的大小和原来的平面的大小是一样的，为了保证特征图的大小和原来的平面的大小是一样的，就会给特征图外面一圈补上0；补多少零要看具体的情况；如果是valid padding, 那么不会超出采样平面的外部，卷积窗口采样后得到一个比原来平面小的平面；

对于池化的操作：same padding:可能会给平面外部补0，valid padding: 不会超出平面外部；

CNN的结构：卷积层+池化层— 卷积层+池化层— 卷积层+池化层 …… — 最后连接一些全连接层，最后得到一个分类





























































