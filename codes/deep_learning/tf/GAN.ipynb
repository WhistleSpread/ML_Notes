{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #导入tensorflow\n",
    "from tensorflow.examples.tutorials.mnist import input_data #导入手写数字数据集\n",
    "\n",
    "import numpy as np #导入numpy\n",
    "import matplotlib.pyplot as plt #plt是绘图工具，在训练过程中用于输出可视化结果\n",
    "import matplotlib.gridspec as gridspec #gridspec是图片排列工具，在训练过程中用于输出可视化结果\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(saver, sess, logdir, step):\n",
    "    \"\"\"用于保存模型\"\"\"\n",
    "    model_name = 'GAN_model'\n",
    "    # 模型的保存路径为\"logdir + GAN_model\"\n",
    "    checkpoint_path = os.path.join(logdir,model_name)\n",
    "    # 保存模型\n",
    "    saver.save(sess, checkpoint_path, global_step = step)\n",
    "    print(\"the checkpoint has been created\")\n",
    "\n",
    "def xavier_init(size):\n",
    "    \"\"\"初始化参数时使用xavier_init\"\"\"\n",
    "    in_dim = size[0]\n",
    "    # 初始化标准差\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    # 返回初始化的结果\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)\n",
    "\n",
    "# X表示真的样本(即真实的手写数字)\n",
    "X = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "\n",
    "#表示使用xavier方式初始化的判别器的D_W1参数，是一个784行128列的矩阵\n",
    "D_W1 = tf.Variable(xavier_init([784, 128]))\n",
    "#表示全零方式初始化的判别器的D_1参数，是一个长度为128的向量\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "D_W2 = tf.Variable(xavier_init([128, 1]))\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]))\n",
    "\n",
    "#theta_D 表示判别器的可训练参数集合\n",
    "theta_D = [D_W1, D_W2, D_b1, D_b2]\n",
    "\n",
    "# Z表示生成器的输入(在这里是噪声)，是一个N列100行的矩阵\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100])\n",
    "\n",
    "# 表示使用xavier方式初始化的生成器的G_W1参数，是一个100行128列的矩阵\n",
    "G_W1 = tf.Variable(xavier_init([100, 128]))\n",
    "# 表示全零方式初始化的生成器的G_b1参数，是一个长度为128的向量\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "G_W2 = tf.Variable(xavier_init([128, 784])) \n",
    "G_b2 = tf.Variable(tf.zeros(shape=[784]))\n",
    "\n",
    "#theta_G表示生成器的可训练参数集合\n",
    "theta_G = [G_W1, G_W2, G_b1, G_b2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    \"\"\" 生成维度为[m, n]的随机噪声作为生成器G的输入\"\"\"\n",
    "    return np.random.uniform(-1., 1., size=[m, n])\n",
    "\n",
    "def generator(z): \n",
    "    \"\"\"\n",
    "    # 生成器，z的维度为[N, 100]\n",
    "    # 输入的随机噪声乘以G_W1矩阵加上偏置G_b1，G_h1维度为[N, 128]\n",
    "    # G_h1乘以G_W2矩阵加上偏置G_b2，G_log_prob维度为[N, 784]\n",
    "    # G_log_prob经过一个sigmoid函数，G_prob维度为[N, 784]\n",
    "    \"\"\"\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "    return G_prob #返回G_prob\n",
    "\n",
    "def discriminator(x): \n",
    "    \"\"\"\n",
    "    #判别器，x的维度为[N, 784]\n",
    "    # 输入乘以D_W1矩阵加上偏置D_b1，D_h1维度为[N, 128]\n",
    "    # D_h1乘以D_W2矩阵加上偏置D_b2，D_logit维度为[N, 1]\n",
    "    # D_logit经过一个sigmoid函数，D_prob维度为[N, 1]\n",
    "    \"\"\"\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1) \n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2 \n",
    "    D_prob = tf.nn.sigmoid(D_logit)\n",
    "    return D_prob, D_logit #返回D_prob, D_logit\n",
    "\n",
    "def plot(samples):\n",
    "    \"\"\"\n",
    "    #保存图片时使用的plot函数\n",
    "    #初始化一个4行4列包含16张子图像的图片\n",
    "    #调整子图的位置\n",
    "    #置子图间的间距\n",
    "    #依次将16张子图填充进需要保存的图像\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(4, 4)) \n",
    "    gs = gridspec.GridSpec(4, 4) \n",
    "    gs.update(wspace=0.05, hspace=0.05) \n",
    " \n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(28, 28), cmap='Greys_r')\n",
    " \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "the checkpoint has been created\n",
      "Iter: 0\n",
      "D loss: 1.477\n",
      "G_loss: 2.653\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 1000\n",
      "D loss: 0.003478\n",
      "G_loss: 8.079\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 2000\n",
      "D loss: 0.0337\n",
      "G_loss: 5.459\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 3000\n",
      "D loss: 0.02842\n",
      "G_loss: 5.474\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 4000\n",
      "D loss: 0.1003\n",
      "G_loss: 5.634\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 5000\n",
      "D loss: 0.1633\n",
      "G_loss: 5.117\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 6000\n",
      "D loss: 0.4167\n",
      "G_loss: 4.639\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 7000\n",
      "D loss: 0.4705\n",
      "G_loss: 4.726\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 8000\n",
      "D loss: 0.4253\n",
      "G_loss: 3.835\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 9000\n",
      "D loss: 0.4615\n",
      "G_loss: 3.445\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 10000\n",
      "D loss: 0.6031\n",
      "G_loss: 2.937\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 11000\n",
      "D loss: 0.538\n",
      "G_loss: 2.671\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 12000\n",
      "D loss: 0.9532\n",
      "G_loss: 2.317\n",
      "()\n",
      "the checkpoint has been created\n",
      "Iter: 13000\n",
      "D loss: 0.6642\n",
      "G_loss: 2.537\n",
      "()\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-81a33a3fee69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m#下面是得到训练一次的结果，通过sess来run出来\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdreal_loss_sum_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfake_loss_sum_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_sum_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mD_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdreal_loss_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdfake_loss_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss_sum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss_curr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss_sum_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mG_solver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss_sum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msample_Z\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmb_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#每过100次记录一下日志，可以通过tensorboard查看\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "G_sample = generator(Z) #取得生成器的生成结果\n",
    "D_real, D_logit_real = discriminator(X) #取得判别器判别的真实手写数字的结果\n",
    "D_fake, D_logit_fake = discriminator(G_sample) #取得判别器判别的生成的手写数字的结果\n",
    " \n",
    "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real, labels=tf.ones_like(D_logit_real))) #对判别器对真实样本的判别结果计算误差(将结果与1比较)\n",
    "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.zeros_like(D_logit_fake))) #对判别器对虚假样本(即生成器生成的手写数字)的判别结果计算误差(将结果与0比较)\n",
    "D_loss = D_loss_real + D_loss_fake #判别器的误差\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake, labels=tf.ones_like(D_logit_fake))) #生成器的误差(将判别器返回的对虚假样本的判别结果与1比较)\n",
    " \n",
    "dreal_loss_sum = tf.summary.scalar(\"dreal_loss\", D_loss_real) #记录判别器判别真实样本的误差\n",
    "dfake_loss_sum = tf.summary.scalar(\"dfake_loss\", D_loss_fake) #记录判别器判别虚假样本的误差\n",
    "d_loss_sum = tf.summary.scalar(\"d_loss\", D_loss) #记录判别器的误差\n",
    "g_loss_sum = tf.summary.scalar(\"g_loss\", G_loss) #记录生成器的误差\n",
    " \n",
    "summary_writer = tf.summary.FileWriter('./logs/snapshots/', graph=tf.get_default_graph()) #日志记录器\n",
    " \n",
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=theta_D) #判别器的训练器\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=theta_G) #生成器的训练器\n",
    " \n",
    "mb_size = 128 #训练的batch_size\n",
    "Z_dim = 100 #生成器输入的随机噪声的列的维度\n",
    " \n",
    "mnist = input_data.read_data_sets('./MNIST_data', one_hot=True) #mnist是手写数字数据集\n",
    "\n",
    "sess = tf.Session() #会话层\n",
    "sess.run(tf.global_variables_initializer()) #初始化所有可训练参数\n",
    " \n",
    "if not os.path.exists('./logs/out/'): #初始化训练过程中的可视化结果的输出文件夹\n",
    "    os.makedirs('./logs/out/')\n",
    " \n",
    "if not os.path.exists('./logs/snapshots/'): #初始化训练过程中的模型保存文件夹\n",
    "    os.makedirs('./logs/snapshots/')\n",
    "\n",
    "saver = tf.train.Saver(var_list=tf.global_variables(), max_to_keep=50) #模型的保存器\n",
    " \n",
    "i = 0 #训练过程中保存的可视化结果的索引\n",
    " \n",
    "for it in range(1000000): #训练100万次\n",
    "    if it % 1000 == 0: #每训练1000次就保存一下结果\n",
    "        samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    " \n",
    "        fig = plot(samples) #通过plot函数生成可视化结果\n",
    "        plt.savefig('./logs/out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight') #保存可视化结果\n",
    "        i += 1\n",
    "        plt.close(fig)\n",
    " \n",
    "    X_mb, _ = mnist.train.next_batch(mb_size) #得到训练一个batch所需的真实手写数字(作为判别器的输入)\n",
    " \n",
    "    #下面是得到训练一次的结果，通过sess来run出来\n",
    "    _, D_loss_curr, dreal_loss_sum_value, dfake_loss_sum_value, d_loss_sum_value = sess.run([D_solver, D_loss, dreal_loss_sum, dfake_loss_sum, d_loss_sum], feed_dict={X: X_mb, Z: sample_Z(mb_size, Z_dim)})\n",
    "    _, G_loss_curr, g_loss_sum_value = sess.run([G_solver, G_loss, g_loss_sum], feed_dict={Z: sample_Z(mb_size, Z_dim)})\n",
    " \n",
    "    if it%100 ==0: #每过100次记录一下日志，可以通过tensorboard查看\n",
    "        summary_writer.add_summary(dreal_loss_sum_value, it)\n",
    "        summary_writer.add_summary(dfake_loss_sum_value, it)\n",
    "        summary_writer.add_summary(d_loss_sum_value, it)\n",
    "        summary_writer.add_summary(g_loss_sum_value, it)\n",
    " \n",
    "    if it % 1000 == 0: #每训练1000次输出一下结果\n",
    "        save(saver, sess, './logs/snapshots/', it)\n",
    "        print('Iter: {}'.format(it))\n",
    "        print('D loss: {:.4}'. format(D_loss_curr))\n",
    "        print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
