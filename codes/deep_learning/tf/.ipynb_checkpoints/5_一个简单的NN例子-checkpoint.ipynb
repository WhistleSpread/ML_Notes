{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 根据一个零件的体积和重量来预测零件是否合格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造需要用到的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表示一次喂入神经网络多少数据，这个数据不能过大\n",
    "BATCH_SIZE = 8\n",
    "seed = 23455\n",
    "\n",
    "# 利用随机种子生成数据集, 基于seed 产生随机数, numpy.random.RandomState()是一个伪随机数生成器\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "# 随机数返回32行2列的矩阵，表示32组 体积和重量 作为输入数据集\n",
    "X = rng.rand(32, 2)\n",
    "y = rng.rand(4)\n",
    "# print(\"X = \", X)\n",
    "# print(\"y = \", y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构造标签Y\n",
    "从X这个32行2列的矩阵中 取出一行， 判断如果两个数的和小于1 则给Y赋值1，如果不小于1，则给Y赋值0 ；Y作为输入数据集的标签(正确答案)\n",
    "实现数据标注，构建数据集, 使用列表推导式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = [[int(x0 + x1 < 1)] for (x0, x1) in X]\n",
    "# print(\"X:\\n\", X)\n",
    "# print(\"Y:\\n\", Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义前向传播的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义神经网络的输入、参数和输出，定义前向传播过程\n",
    "# x 是输入的特征，\n",
    "# 先用placeholder 占位，表示x可以多个输入，每次输入2个元素, y_表示输出，可以有多个输出，输出1个元素\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, 2))\n",
    "y_ = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "\n",
    "# 将需要训练的参数w1, w2，设置为变量，输入为2，隐藏层为3，输出为1，最开始这些参数用随机数来生成初始值\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([2, 3], stddev=1, seed=1))\n",
    "w2 = tf.Variable(tf.random_normal([3, 1], stddev=1, seed=1))\n",
    "\n",
    "# 构建神经网络，其实就是矩阵的乘法\n",
    "\n",
    "a = tf.matmul(x, w1)\n",
    "y = tf.matmul(a, w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义损失函数以及训练方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数及反向传播方法，损失函数为mean square error, 训练方法为梯度下降法\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y - y_))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "# 训练方法也可以考虑使用其他的方法\n",
    "# train_step = tf.train.MomentumOptimizer(0.001, 0.9).minimize(loss)\n",
    "# train_step = tf.train.AdamOptimizer(0.001).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成会话，初始化所有的变量，训练Steps轮， 分组训练，一组8个，也就是一次喂入8个，每500轮打印一次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 training step(s), loss on all data is 5.13118\n",
      "After 500 training step(s), loss on all data is 0.429111\n",
      "After 1000 training step(s), loss on all data is 0.409789\n",
      "After 1500 training step(s), loss on all data is 0.399923\n",
      "After 2000 training step(s), loss on all data is 0.394146\n",
      "After 2500 training step(s), loss on all data is 0.390597\n",
      "('w1:\\n', array([[-0.7000663 ,  0.9136318 ,  0.08953571],\n",
      "       [-2.3402493 , -0.14641267,  0.58823055]], dtype=float32))\n",
      "('w2:\\n', array([[-0.06024267],\n",
      "       [ 0.91956186],\n",
      "       [-0.0682071 ]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    sess.run(init_op)\n",
    "    \n",
    "#     # 输出目前（未经训练）的参数取值，也就是优化前的参数\n",
    "#     print(\"w1:\\n\", sess.run(w1))\n",
    "#     print(\"w2:\\n\", sess.run(w2))\n",
    "    \n",
    "    # 训练模型, 训练3000次，每次为一组：8个\n",
    "    \n",
    "    STEPS = 3000\n",
    "    for i in range(STEPS):\n",
    "        start = (i * BATCH_SIZE) % 32\n",
    "        end = start + BATCH_SIZE\n",
    "#         print(\"i = \", i, \"start = \", start, \"end = \", end)\n",
    "\n",
    "        # 每一次都喂入8组数组\n",
    "        sess.run(train_step, feed_dict={x:X[start:end], y_:Y[start:end]})\n",
    "        \n",
    "        # 每500轮打印一次loss值\n",
    "        if i % 500 == 0:\n",
    "            total_loss = sess.run(loss, feed_dict={x:X, y_:Y})\n",
    "            print(\"After %d training step(s), loss on all data is %g\" %(i, total_loss))\n",
    "  \n",
    "    # 输出训练后的参数取值\n",
    "    print(\"w1:\\n\", sess.run(w1))\n",
    "    print(\"w2:\\n\", sess.run(w2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
