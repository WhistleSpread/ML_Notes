### Sequential models

目前，我们已经将我们的注意力放在了那些每一个输出y被认为是由相关的输入x所产生的，而且我们的hypothesis都是一些纯函数，在这些纯函数中，输出只取决于输入(而且我们所学习出来的参数控制着函数的行为)，在接下来的几周时间里，我们所考虑的模型将超越函数，远没有函数那么简单；

- 在循环神经网络中，我们所学习的hypothesis，并不是关于一个单一输入的一个函数了，rnn的输入是一个输入序列，这个输入序列是这个predictor所获得的

  > 这个是什么意思，输入序列是predictor has received ? 啥意思？？？

- 在强化学习当中，我们所学习到的hypothesis,要么是一个模型(of a domain)(比如说一个博弈)， 这个模型作为一个循环的系统，要么就是一个policy，这个policy 是一个纯函数，但是这个policy的损失函数的确定是通过policy 与 domain的交互作用来确定出来的；

  > 这个也没有看明白，什么是domain, 什么是policy, 学完之后回过头再来看；



### 1. 状态机

所谓状态机呢，就是对一个过程的描述，这个过程可以是计算过程，可以是物理过程，也可能是经济过程，状态机这个概念是一个相当广泛的概念，在不同的领域具有不同的名字，状态机是根据潜在的状态序列来过程的.

一个系统的状态被定义为是系统的状态被定义为您现在需要的关于系统的所有信息，以便尽可能地预测其未来的轨迹。状态可以是一个物体的位置或者速度的信息，或者是你在棋盘上的位置，或者是当前高速公路上的交通流量密度；

现在我们比较正式的来定义一个状态机$(S, x, y, s_0, f, g)$

- $S$ 是可能的状态的有限或者无限集合

- $x$ 表示有限或者无限可能的输入

- $y$ 表示有限或者无限的输出

- $s_0 \in S$表示状态机的初始状态

- $f: S \times x \rightarrow S$ 表示状态转移方程，这个方程输入的是上一个状态以及输入，然后输出的是下一个状态；

- $g: S \rightarrow y$ 是一个输出函数，这个函数接受一个状态，然后产生一个输出值；

  状态机的基本操作是首先从初始状态$s_0$出发，然后迭代的计算：
  $$
  s_t = f(s_{t-1}, x_t) \\
  y_t = g(s_t)
  $$

因此，给了a sequence of inputs $x_1, x_2, …$, 相应的状态机就会产生 a sequence of outputs
$$
g(f(x_1, s_0)), g(f(x_2, f(x_1, s_0))), ...
$$
我们有时候会这样来表示：这个状态机将序列x转换成了序列y, 在时间点t输出的结果与步骤1到t都是有关的；

一个比较常见的形式就是有限状态机infinite state machines, 在这个有限状态机中$S, X, Y$都是有限集合，有限状态机通常使用状态转移图来进行描述，在这个状态转移图中，节点表示状态，而弧表示转移，我们在节点上标记的是这些节点产生的输出结果，弧线上的标记表示的是哪一个输入导致的这种状态转移；

另外一个简单但是强大的用在信号处理和控制的就是线性时间不变(LTI)系统，在这种情况下，状态$S = R^m$, 输入$X = R^l$, 输出$Y = R^n$, $f$与$g$都是关于他们的线性函数，在离散的时间点上，他们可以被定义为线性差分方程，就像:
$$
y[t] = 3y[t-1] + 6y[t-2] + 5x[t] + 3x[t-2]
$$
可以使用状态来实现存储之前相关的输入，以及信息的输出；

我们会研究循环神经网络，这个循环神经网络非常像非线性版本的LTI系统，rnn的状态转移方程：
$$
f(s, x) = f_1(W^{sw}x + W^{ss}s +W_0^{ss})
$$
输出函数:
$$
g(s) = f_2(W^0s + W_0^0)
$$
x: $\ell \times 1$

s: $m \times 1$

$f_1$与$f_2$都是激活函数，实际上是有可能使用梯度下降来训练权重值的



### 2. 马尔可夫决策过程

马尔可夫决策过程是状态机的一个变种，在马尔可夫决策过程中:

- 状态转移方程是随机的，这也就意味着它是定义了一个概率分布，这个概率分布是是考虑在给定前一个状态以及输入的条件下，考虑下一个状态，但每次评估时，它都会从该分布中得到一个新状态。
- 输出等于状态(换句话说，这里的g是identity  function)
- 某些状态(or state-action pair) are more desirable than others

一个马尔可夫决策过程







#### 2.1 Finite-horizon solutions

##### 2.1.1 Evaluation a given policy

##### 2.1.2 Finding an optimal policy



#### 2.2 Infinite-horizon solutions

##### 2.2.1 Evaluation a given policy
##### 2.2.2 Finding an optimal policy

##### 2.2.3 Theory







