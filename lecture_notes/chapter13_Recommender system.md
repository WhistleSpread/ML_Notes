### 推荐系统

推荐系统应用非常的广泛，而且推荐系统是一个非常有意思的领域，不仅仅是因为它有几个公式(it has several formulations), 而且有些公式会应用到问题的特定的有意思的结构；

举个例子：我们可以思考一下像Nexflix这样的公司，这个公司为它的用户推荐电影，网飞公司知道许多不同的用户给许多不同的电影的的评价，网飞公司也知道你对你看过的一些电影的评价，那么Netflix公司如何根据这些数据来为你提供今晚的推荐电影呢？

要解决这个问题，有两种普遍的做法：第一种方法是基于内容的推荐，这种方法其实可以看成是一种有监督的学习问题；第二种方法是采用系统过滤的方法；这种协同过滤是一种新的学习的思路；

#### 1. 基于内容的推荐

前面我们说了，基于内容的推荐是一种有监督的学习，在这种基于内容的推荐学习模型中，我们要学习一个**预测器**， 我们暂时把它叫做predictor f，这个预测器是用你之前所评价的电影数据作为训练数据来找到一个假设hypothesis， 这个预测器predictor f映射的就是对任意的某一个电影，预测出你可能对这个电影的评价；最后，Netflix公司就会返回那些你可能打高评分的电影；

我们做的第一步就是设计输入和输出的数据表示representation;

实际上，要对一个电影设计出好的特征表示是相当难的，一种比较合理的设计特征表示的方法或许是基于电影的类型，时长，主演，导演，片源地，甚至是某些标准评论家给出的评价，或者是某些aggregation sources(我不知道这个aggregation sources是个啥)，通过这些过程，就可以将一部电影映射称为一个向量；
$$
\phi : movie \rightarrow vector
$$
一般，对于电影的评分，往往是用几颗星，几颗星来表示，我们这里假定是5颗星，所以输出的值域可能是$\{1, 2, 3, 4, 5\}$, 对于这种输出，采用one-hot来编码是不太合适的，并且我们假定这些值是实数值并不完全有道理，然而，我们目前就是把这些值看成是实数值的；其实这里采用Thermometer coding 可能更加合理一些，但是只是口头上说，而没有具体的去实验一下很难具有说服力；

>  为什么采用独热码这种编码方法并不好？为什么说是实数值上的并不是一个好的选择？

现在，我们得到了一个编码方法，我们可以基于你之前对电影的评分来获得一个训练集；

接下来我们要做的就是挑选出一个损失函数，这个损失函数的选择实际上与输出编码的选择关系非常密切。因为我们决定将输出看作是一个实数，实数是连续的，所以我们可以将这个问题看成是一个回归问题，也就是要将输入的向量映射到一个实数值，所以我们的损失函数选择的就是mse, $Loss(p,y) = \frac 1 2 (y - p )^2$, 一般情况下我们需要进行正则化，因为用于训练的数据集往往比较小，除非你真的看了非常多的电影，否则非常容易过拟合；

最后，我们需要挑选出一个hypothesis space, 最简单的当然是让它是一个线性的就好了；但是除了线性的，我们当然还可以有更多的想法，比如说，我们可以使用神经网络；

如果我们将前面所述的特征表达，损失函数，正则化，假设空间都放在一起，那么最后，我们的目标函数就是下面这个样子：
$$
J(\theta) = \frac 1 2 \sum_{i \in D_{\alpha}}(y^{(i)} - \theta^Tx^{(i)} - \theta_0)^2 + \frac \lambda 2 {||\theta||}^2
$$
这个不是我们的老朋友岭回归吗？这个既有解析解，也可以使用梯度下降进行求解；



#### 2. 协同过滤 Collaborative filtering

其实，在前面使用基于内容的推荐系统中存在两个比较难处理的地方：

- 很难设计一个好的特征集来表征一部电影(**可不可以想办法解决这个问题呢？？？**)
- 基于内容的推荐系统只是使用你之前看过的的电影的评分，但是无法使用其余的数据(**这个地方没有搞懂，为什么要用其他的数据？？？**)

在协同过滤这种方法中，我们会尝试使用其他人对所有电影的评分来帮助你做出更好的预测；

从直觉上来讲，我们可以将这个过程看作是找到那些和我具有相同的电影喜好的那些人，然后就可以预测说对于他们喜欢的电影，我也会喜欢；

首先，我们通过构建一个数据矩阵$Y$, 在这矩阵中的某一个元素$Y_{ai}$代表的是用户$a$对电影$i$的评分，因此，如果我们有$n$个用户$m$个电影，那么我们得到的矩阵的形状就是$m \times n$; 

> 在实际中，我们并不会将所有的数据着这么来呈现，否则这个矩阵又大又稀疏，我们就这么来想想就好；

因为$Y$这个矩阵具有稀疏性，为了克服这种稀疏性，我们可以将我们的训练集看成是三元组的集合$D = \{ (a, i, r)\}$, 在这里$a$ 代表的是特定用户的索引，$i$代表的是特定电影的索引，$r$是用户$a$对电影$i$的评分；我们接下来要做的事情就是要想一个办法，使用这个集合$D$来预测这个稀疏矩阵中的值；我们让$X$ 表示我们预测出来的评分矩阵，现在我们需要找到一个关于$X$和$Y$的损失函数, 我们通过优化这个损失函数来找到一个好的预测模型；

##### idea 1:  

我们用我们之前的方法来设计我们的损失函数；我们可能会想说我们预测出的$X_{ai}$应该要和$Y_{ai}$的值越一致越好，然后我们再加上一些正则项，这样一来，损失函数就是下面这个样子:
$$
Loss(X, Y) = \frac 1 2 \sum_{(a,i)\in D}(Y_{ai} - X_{ai})^2 + \sum_{all(a, i)}X_{ai}^2
$$
这个想法其实是错误的想法，因为如果这么干的话，这个损失函数会使得$X_{ai} = 0 $ 对于所有的$(a, i) \notin D$ 

我们需要找到一个不同类型的正则化方法，这种正则化的方法将会强迫某些泛化到未见 force some generalization to unseen entries.(**这句话没有看明白**) 

线性代数的思维: 一个矩阵的秩是这个矩阵中最大的线性无关的行的个数(当然也等于线性无关的列的个数)；

如果一个$n \times m$的矩阵$X$的秩为1， 那么存在$\mathcal U$和$\mathcal V$, 他们形状分别是$n \times 1$ 和$m \times 1$, 使得
$$
X = \mathcal U \mathcal V^T
$$
如果$X$ 的秩为k，那么 那么存在$\mathcal U$和$\mathcal V$, 他们形状分别是$n \times k$ 和$m \times k$, 使得：
$$
X = \mathcal U \mathcal V^T
$$

##### idea2:

我们要找一个秩为1的矩阵$X$， 这个X中的元素应该尽可能的fit$Y$中的元素。**这里我没有看明白，为什么要找秩为1的矩阵呢？要求这个矩阵只有1个独立的行，只有1个独立的列？我们的目的是什么？这里没有搞明白，先这么看着吧，反正就是找一个秩为1的矩阵，在思考之后我明白了，之所以要找秩为1的矩阵，目的就是为了尽量产生多的冗余，这样有利于我们将空白处填充出来**， 相比于上一个表达，这个表达是一个更加低维的表达(这个表达有m+n维，而不是mn维)，相同的参数在许多predictions之间是共享的；因此这个想法比之前的想法具有更好的泛化性；

因此，我们需要找到向量$\mathcal U$和$\mathcal V$,使得:
$$
\mathcal U \mathcal V^T = X
$$
因为我们使用的是求均方误差，因此我们的目标函数:
$$
J(\mathcal U, \mathcal V) = \frac 1 2 \sum_{(a, i)\in D}(\mathcal U^{(a)}\mathcal V^{(i)} - Y_{ai})^2
$$
现在我们如何来找到最优的$\mathcal U$和$\mathcal V$呢？我们可以将目标函数分别对$\mathcal U$和$\mathcal V$分别求导数；对于每一个参数$\mathcal U^{(a)}$或者是$\mathcal V^{(i)}$, 都会得到一个像这样的等式，我们不知道对这组方程组如何快速得到一个解析解， because the parameters U and V are multiplied by one another in the predictions(因为参数U和V在预测中相互相乘), so the model does not have a linear dependence on the parameters. **所以模型没有对参数的线性依赖性** ， 我们可以使用梯度下降来解决这个问题，在下一节，我们会使用梯度下降来解决一个相关的模型；

但是，在我们讨论最优化之前，首先来思考一下，这个模型是如何表示的，对于每一个用户都具有一个参数(向量$\mathcal U$中的元素)， 对于每一部电影都有一个参数(向量$\mathcal V$中的元素)，对于预测的评分是预测评级是这两个数的乘积。；It can really represent only each user’s general enthusiasm and each movie’s general popularity, and predict the user’s rating of the movie to be the product of these values.



##### idea 3

如果我们使用秩为1的矩阵分解还不够具有表达性，或许我们可以尝试一下使用秩为k的矩阵分解；如果我们使用秩为k的矩阵分解的话，那么我们就要找一个$n \times k$的矩阵$\mathcal U$和一个$m \times k$ 的矩阵$\mathcal V$, 目标函数依然是我们之前的公式7；这里的$X_{ai}$就是矩阵$\mathcal U$的第$a$行和矩阵$\mathcal V$的第i列做内积；是矩阵$\mathcal U$的第$a$行代表的是用户$a$的$k$ 个特征；相应的矩阵$\mathcal V$的第i列表示的是电影$i$的k个特征；在这种情况下，我们需要预测的总的参数就是$nk + mk$个，但是这个是一种冗余的表达方式，因为当k=1的时候，推广到一般情况就是$k^2$, 因此，实际上真正起作用的，也就是所谓的自由度，其实是$mk + nk - k^2$; 

在我们的这个应用预测中加上一些偏移量仍然是有用的，因此我们会考虑加上一个$n \times 1$的向量$b_{\mathcal U}$作为偏移量，考虑加上一个$m \times 1$的向量$b_{\mathcal V}$作为偏移量；然后加上对$\mathcal U$和$\mathcal V$的正则化，因此最后我们的目标函数就是:
$$
J(\mathcal U, \mathcal V) = \frac 1 2 \sum_{(a, i)\in D}(\mathcal U^{(a)}\mathcal V^{(i)} + b_{\mathcal U}^{(a)}+ b_{\mathcal V}^{(i)}- Y_{ai})^2 + \frac \lambda 2 \sum_{a = 1}^n{||\mathcal U^{(a)}||}^2 + \frac \lambda 2 \sum_{i = 1}^m{||\mathcal V^{(i)}||}^2
$$

> 思考一下，对于这两个偏移量的非正式的解释是什么比较的合理？





#### 2.1 优化模型

现在，我们得到了目标函数了，接下来要做的事情就是如何来优化这个目标函数了；实际上存在两个比较可行的方法来找到$\mathcal U, \mathcal V, b_{\mathcal U}, b_{\mathcal V}$, 一种方法是使用:alternating 最小二乘，这种方法是构建在我们的线性回归的解析解上面的，另外一种方法是随机梯度下降的方法，这种方法我们在神经网络和其他模型中早就用过了；

#### 2.1.1 Alternating least squares

应该值得注意的一件有意思的事情就是，如果你固定住$\mathcal U$和$b_{\mathcal U}$, 然后去找$\mathcal V$和 $b_{\mathcal V}$ 这样的问题其实就是一个线性回归问题，对于线性回归问题，我想你应该知道如何来解决； 同样的，如果我们固定$\mathcal V$和$b_{\mathcal V}$，然后去找$\mathcal U$和$b_{\mathcal U}$, 使目标函数最小化，那么这个问题也是类似线性回归的问题；因此，我们会考虑一个算法，这个算法交错的使用下面的形式:我们固定住$\mathcal U$和$b_{\mathcal U}$, 最开始的取值是随机取值，然后找到在这个固定的$\mathcal U$和$b_{\mathcal U}$, 下最好的$\mathcal V$和 $b_{\mathcal V}$, 然后固定$\mathcal V$和 $b_{\mathcal V}$, 然后再去找到最好的$\mathcal U$和$b_{\mathcal U}$, 就像这样一直循环下去；

其实这个也是一种最优化的方法，这种方法被称为是"坐标下降法"， 因为我们在一次只改进这个模型的一个坐标(或者，在这里是一组坐标)，一般情况下，坐标下降法有类似梯度下降法的属性，并且坐标下降法不能保证说我们一定能够找到一个全局最优；在这个问题当中，我们采取坐标下降法其实是一个非常可取的做法；因为在我们这个问题中，当一个坐标被固定的时候，也就是说只有一个坐标变动的时候，这个问题其实就简化成了我们所熟悉的线性回归的问题；

更加具体的操作如下:

1. 我们首先随机的初始化$\mathcal V$和$b_{\mathcal V}$

2. 对于每个$a$ 在1, 2, …, n中

   - 构建一个线性回归问题，这个问题是要找到一个$\mathcal U^{(a)}$, 来最小化
     $$
     \frac 1 2 \sum_{(a, i)\in D}(\mathcal U^{(a)}\mathcal V^{(i)} + b_{\mathcal U}^{(a)}+ b_{\mathcal V}^{(i)}- Y_{ai})^2 + \frac \lambda 2 \sum_{a = 1}^n{||\mathcal U^{(a)}||}^2
     $$

   - 回忆一下最小二乘的目标(在下面的描述中，我们会忽略偏移量与正则项，这样更加容易看清楚基本的思想):
     $$
     (\bold W\theta - T)^T(\bold W\theta - 
     $$
     
     在这种情况下，

4. 交替使用步骤2和3，最优化向量$\mathcal U$和$\mathcal V$, 然后经过有限次迭代，或者两个连续参数的之间的差值足够的小；然后就停止迭代；



##### 2.1.2 随机梯度下降

最后，我们会使用随机梯度下降的方法来解决这个问题；































