#### 神经网络

除非你是住在山洞里的原始人，否则你不可能没有听说过神经网络；现在，我们已经学过了一些有用的机器学习 的概念了，(hypothesis class, 分类，回归，梯度下降，正则化等)， 现在，我们有足够的实力来理解什么是神经网络了；

在某种意义上来讲，现在我们所说的神经网络实际上是第三波神经网络；其实神经网络的基本思想早在1943年就The basic idea is founded on the 1943 model of neurons of McCulloch and Pitts and learning ideas of Hebb. 尽管在当时引起了轰动，可是并没有太多的实践成功的例子；在当时，对于训练出线性函数是存在一些比较好的方法的，比如说(感知机)， 然而在当时也有许多非线性分类的函数，可是在当时并没有很好的方法能够从数据中训练出非线性的模型(函数), 当时人们对于想要从数据中训练处非线性模型的热情有所下降，但是在20世纪80年代，人们的这种训练出非线性模型的热情再一次被点燃，因为在当时，有几个人用提出了使用一种叫做“反向传播”的方法来训练神经网络，反向传播是一种实现梯度下降的一种比较特殊的方式；我们在之后会学到；

> 正如科学中许多好的想法的诞生一样，关于如何使用梯度下降的方法来训练非线性的神经网络模型的基本思想其实是被多个人独立的开发出来的；

直到90年代中期，人们对于神经网络的热情再一次消退，因为，尽管我们可以训练出非线性的网络出来，这种训练过程往往非常的慢，而且在寻优的过程当中容易陷入到一种局部最优的情况，支持向量机和核方法提供了一套可靠的学习过程，并且支持向量解可以保证收敛，并且不会达到局部最优；

然而，当人们对支持向量机趋之若鹜的时候，有几个小组仍然在研究神经网络模型，而且他们的工作，再结合一些可获得的数据和计算，使得神经网络再一次兴起，这次，神经网络变得更加的可靠，变得更加的强大；现在神经网络这种方法几乎是许多应用领域中的首选；对于神经网络，其实存在着许许多多的变种，到底有多少种呢？你可以在arxiv.org这个网站上去查，在这里，我们会学习核心的"前向传播"网络与”反向传播“训练；然后，在之后的章节中，我们会探讨一些在这些核心技术之上的比较前沿的进步；

我们可以从几个不同的视角来看一下神经网络模型：

视角1: 可以讲神经网络看成是一个随机梯度下降的算法，主要是用于分类和回归；这个神经网络模型具有非常丰富的hypothesis class

视角2: A brain-inspired network of neuron-like computing elements that learn distributed representations.

视角3: 神经网络其实是这样的一个方法，这个方法可以被应用到实际的生成过程当中，这个应用需要我们能够作出预测，基于巨大的数据量，并且是在非常复杂的领域中；

我们在学习的过程中主要是从视角1的角度来学习，我们需要了解的是，我们所开发的这项技术将会被应用到第三个观点中，第二个视角是早期开发出神经网络模型的主要动机，但是我们将会学习的这项技术似乎并没有真正的解释我们大脑中的生物学习过程；

> 实际上，有一些非常有名的研究者们在试图努力的弄清楚这些方法在我们的大脑中到底是如何类似的进行工作的；



#### 1. Basic element

一个神经网络的基本单元就是一个neuron,下面的图表示的就是一个神经元，有时候，我们也会将一个神经元称为是一个单元或者是一个节点node, 神经网络模型本质上是一个非线性的函数，这个函数的输入是一个m维度的向量，输出是一个实数值，他的参数is parameterized by a vector of weights$(w_1, …, w_m) \in R^m$, 以及一个偏移量或者是叫做是个threshold，门限$w_0 \in R$, 因为我们想要我们的神经元是一个非线性的神经元，在这里我们也确定出了一个激活函数activation function f: $R \rightarrow R$, 这个激活函数是将一个实数值映射到另外一个实数值上；我们可以将这个激活函数看成是$f(x) = x$, 当然这个激活函数也可以是任何其他的函数，但是又一点要注意，只有当这个激活函数是可微的时候，我们才能够对这个激活函数做点什么事情；

这个神经元所表示的函数可以被表达成下面这个样子：
$$
\alpha = f(z) = f(\sum_{j=1}^mx_jw_j + w_0) = f(\bold w^T\bold x + w_0)
$$
在我们思考整个神经网络之前，我们可以考虑如何训练a single unit, 假设我们给定了一个损失函数loss function L(guess, actual), 以及数据集$\{ (x^{(1)}, y^{(1)}), …, (x^{(n)}, y^{(n)}) \}$, 我们可以最(随机)梯度下降算法，然后来调整权重$w, w_0$ 来最小化
$$
J(w, w_0) = \sum_\limits{i}L(NN(x^{(i)}; w, w_0), y^{(i)})
$$
在这里NN表示给定一个输入，我们的神经网络给定的输出；我们已经研究了关于神经元的两个特例：一个是带hinge-loss 的线性分类器，一个是带quadratic-loss 的regressors, 这两个都是线性分类器，这两个分类器的激活函数都是$f(x) = x$;

> 一个问题： 如果一个单一的神经元，其激活函数为$f(z) = e^z$, 损失函数$L(g, a) = (g - a)^2$, 推导一下梯度下降的更新公式，for $w, w_0$; 



#### 2 Networks

现在，我们要将许多个神经元组合在一起构成一个神经网络，一般来讲，一个神经网络的输入是一个维度为m的一个向量$x \in R^m$,输出是一个n维的向量$\alpha \in R^n$, 一个神经网络是由许多个神经元组成的，每一个神经元的输入可能是元素$x$ 和/或 其他神经元的输出，这个输出是由n个神经元所产生的；

在这一章节，我们只考虑前向传播网络，在前向传播网络中，你可以将网络看成是是一个函数调用图，这个图是无环的，也就是说，一个神经元的输入是不会依赖于神经元的输出的(因为它是无环的)， 也就是说，数据流的流动是单向的，从输入流向输出，而且整个神经网络所计算的函数只是各个单独的神经元函数的组合；

虽然，神经网络的图结构基本上什么都可以表达(只要能够满足前向传播的约束)，为了在软件设计和分析上的方便，我们通常是将神经网络按照层来设计，一层就是一组神经元，这组神经元基本上是并行的，他们的输入是前一层神经元的输出，而且他们的输出是下一层神经元的输入，我们首先描述一下单一层，然后再描述多层神经网络；

##### 2.1 Single layer

一层就是一组单元，就像我们这里下面画出来的这样，每个单元互相是不相连的，这种层被称为是全连接层；正如下面的图所示，对于每一个神经元来说，输入都是一样的，在这里就是$x_1, x_2, …, x_m$, 每一层输入是一个m维的向量，输出是一个n维的向量；

因为每一个神经单元都有一个权重向量，以及一个偏移量，我们可以将这个单层网络中所有的权重想象成是一个矩阵$\bold W$, 并且所有的偏移量的集合的向量为$\bold W_0$, 如果我们有m个输入，n个输出，那么

- $\bold W$ 是一个$m \times n$的矩阵
- $\bold W_0$ 是一个$n \times 1$的列向量
- $\bold X$, 输入向量，是一个$m \times 1$的列向量
- $\bold Z = \bold W^T\bold X + \bold W_0$, the pre-activation 是一个$n \times 1$的列向量

输出的向量是:
$$
A = f(\bold Z) = f(\bold W^T\bold x + \bold W_0) 
$$
在这里，激活函数被应用到pre-activation 向量的每一个值上。

我们对一个单一层能够做点什么？我们已经看到了这个单一层的神经网络，我们可以看到这个单一层的神经网络的形式就想我们的线性分类器和线性回归器一样，如果我们手头上只有一个单一层的话，single layer, 那么我们所能够做的只是一个linear hypothesis, (当然我们可能对输出做一些非线性的变换)， 我们之所以要构建一个神经网络，就是要朝着非线性的non-linear hypothesis 迈进！！！

> 所以说，对于单一层的神经网络，我们只能得到线性的分类模型，要得到非线性的模型，我们必须增加神经网络的层数；
>
> 不过，既然这里一层神经网络只能预测线性模型，那么那个激活函数的作用到底是什么呢？思考一下，如果没有非线性的激活函数的话，那么增加再多的神经网络层，最后也只不过是一个线性的组合罢了，如果没有激活函数，整个函数将是一个线性函数：所以这个激活函数必须是一个非线性的激活函数；



##### 2.2 Many layers

一个的神经网络模型通常会包含多个层，一般是前一层的输出作为下一层的输入；

我们首先要建立一些术语集合，我们用$l$表示一个神经网络的层数，用$m^l$表示层$l$的输入的个数的数量，用$n^l$表示在层$l$的输出的数量；然后$W^l$和$W_0^l$ 这两个矩阵的形状分别是$m^l \times n^l$, 以及$n^l \times 1$, 我们用$f^l$表示$l$层的激活函数，之后，the pre-activation outputs are the $n^l \times 1$ vector
$$
\bold Z^l = \bold {W^l}^T\bold A^{l-1} + \bold W_0^l
$$
激活函数输出仅仅就是$n^l \times 1$的向量
$$
A^l = f^l(Z^l)
$$

>



#### 3 Choices of activation function

对于激活函数其实有很多的选择，可是，激活函数的选择真的是有必要的吗？如果我们让激活函数是identity呢？如果没有激活函数，那么最后经过若干层，最后的结果依旧只不过是最开始输入的线性组合罢了换句话说，any function representable by any number of linear layers (where f is the identity function) can be represented by a single layer；我们加了这么多层并没有增加整个网络的表示容量representation capacity of the network;换句话说，如果没有激活函数的话，这个多层的神经网络其实只相当于是一层； 这也就说明了，激活函数的非线性性是至关重要的！

> 这里又一点没有想明白，为什么是一层呢？为什么是一层一层的，不是一个一个的呢？为什么不是多层可以由一个来表示，或者多个由一个来表示；

既然我们知道了我们必须要有一个非线性的激活函数，那么这个非线性的激活函数的选择有什么影响吗？我们接下来考虑下面这几个:step function 、Rectified linear unit(ReLU),Sigmoid(logistic，可以被解释成概率，因为sigmoid函数的值取值都是在0到1之间的)、Hyperbolic tangent 这个函数的取值总是在-1到+1之间；最后一个是softmax 函数。

对于softmax 函数，一个softmax函数的输入是一个n维空间的一个向量$\bold Z \in R^n$, 输出会是一个向量$\bold A \in [0, 1]^n$,也就是说输出的也是一个n维的向量，只不过这个n维向量具有下面这种性质:$\sum_{i=1}^nA_i = 1$  这也就意味着我们可以将$\bold A$这个结果转换成n项的概率分布

在最开始神经网络使用的是step函数，作为激活函数的，可是由于这个激活函数不好求导，所以我们之后就不考虑这个函数了；考虑一下，我们可以看到sigmoid 非常像这个step 函数对吧。

> 思考一下，如何在sigmoid函数上增加一个参数来使得sigmoid更像step函数？
>
> 
>
> relu函数的导数是什么？是否会有某个输入值是的relu函数导数消失？ 有啊，就是在原点处；

Relu 激活函数是我们在隐藏层中经常用到的激活函数，而如果输出是一个二分类问题的话，我们的输出层一般使用的是sigmoid 函数，如果输出层是一个多分类问题的话，我们一般在输出层使用的是softmax函数；





