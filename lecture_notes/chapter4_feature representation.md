### Feature representation

我们之前学习的线性分类器其实还算比较简单的一类分类器，但是线性分类器属于那种比较严格的一类假设分类器，如果我们要在一个低维空间中做一个复杂的分类，那么对于线性分类器来说，或多或少有点力不从心；

要说对于线性分类器来说力不从心的例子，我们最喜欢举的例子就是一个异或的数据集"exclusive or"(XOR)， 可以说这个异或数据集就是我们机器学习数据集中的"果蝇"！

对于这个二维平面中的异或数据集，就不存在一个线性分隔器能够将这个数据集给分开；但是我们有一个可用的技巧：我们可以用一个非线性变换将这个低维的数据集映射到高维空间中，然后在这里高维空间中我们就可以找到一个线性分类器将这个异或数据集给分开了；

举个例子：在一个一维的数据集(也就是一条直线)中， 我们有这样的4个点，这些点在一维上并不是线性可分的。

> 因为我们不能找到一个一维分隔器(也就是一个点), 能够将这4个点分开

但是考虑一下，如果存在下面这样一个非线性变换$\phi(x) = [x, x^2]$, 我们将数据放入到这个$\phi$空间中，我们将会看到，现在这4个点在$\phi$空间中是可分的；我们从图中可以看到，有许多可能的separator, 事实上，在$\phi$空间中的一个线性分类器如果映射回原来的一维空间中，它其实是一个非线性的分类器。让我们来看看它是怎么实现的；现在考虑一个非线性的分隔器$x^2 - 1 = 0$, 我们将半平面$x^2 - 1 > 0$ 标记为正，反之将$x^2 - 1 < 0 $标记为负，那么这个$x^2 - 1 = 0$分隔器对应的原来的一维空间中的分隔器是个什么样子呢？要考虑清楚这个问题，我们就要先问一问：在一维空间中的那些点经过映射可以映射到$x^2 - 1 = 0$上？, 答案就是"+1, -1", 因此，在一维空间中，这两个点构成了我们的separator, 我们可以用相同的思路来找到被分类器$x^2 -1$标注为正的那么样本点在一维空间中都对应着那些点；

> 可以看到，标注为正的点要么在1的右侧，要么在-1的左侧；

这是一个非常一般的，非常通用的策略，这个方法的基础来自于**核方法**， 核方法是一个非常强大的方法，不幸的是在本节课上，我们不会深入到核方法中；除此之外，这种将低维不可分数据映射到高维可分，也可以看作是神经网络中增加多个隐藏层的动机；也就是说，为什么神经网络中要加入隐藏层呢？因为加入了隐藏层后，原来不可分的变得可分了，就可以像这么来理解；

当然了，有许多不同的方式来构建我们的映射函数$\phi$, 有些方法相对系统，并且是某个独立的领域的方法，我们在接下来的第一部分会看一下**polynomial basis**, 也就是基于多项式的方法来映射核函数，其他与原始特征的语义（含义）直接相关，我们在构造它们时考虑到了相关背景领域， 我们在第二部分的时候会讨论这些策略。



#### 1. Polynomial basis

如果在你的问题里面，这些特征都本身就是数值型的数据，那么一种比较系统的构建一个新的特征空间的方法就是使用**Polynomial basis**, 基于多项式方法的一个基本思想就是如果你使用k阶的基，(k-th basis) (k 是一个正整数)那么在你新生成的特征空间中就会包含在你原始的输入数据中所有的k个不同维度乘积。

下面这张表就解释了k-th order polynomial basis 对于不同的k值

| order k | d = 1 | 一般情况 |
| :-: | :--: | :--: |
| 0   | $[1]$ | [1] |
| 1  | $[1, x]$ | $[1, x_1, …, x_d]$ |
| 2   | $[1, x, x^2]$ | $[1, x_1, …, x_d, x_1^2, x_1x_2, x_1x_3, …, x_{d-1}x_d]$ |
| 3 | $[1, x, x^2, x^3]$ | $[1, x_1, …, x_d, x_1^2, x_1x_2, x_1x_3, …, x_{d-1}x_d,x_d^2, x_1^3, …, x_1x_2x_3…x_3^3]$ |

> 这里的k-th basis 就是将数据映射到k维空间中；这里表述有问题，不是k维空间吧。。。

那么，如果我们想要求解这个异或问题，我们就可以使用一个polynomial basis 作为一个特征转换函数，这样一来，我们就可以将我们的二维数据集转换成为更高维度的数据集，通过使用函数$\phi$; 现在，像之前一样，我们手上有一个分类问题需要解决，我们可以使用我们之前学过的感知机算法来解决这个分类问题。

现在还是考虑我们之前的那个异或的问题(XOR)，我们使用上面的那个Polynomial basis 的特征转换函数，我们见k设置为2，那么这个特征转换函数如下：
$$
\phi((x_1, x_2)) = (1, x_1, x_2, x_1^2, x_1x_2, x_2^2)
$$

> 思考一个问题：如果在进行特征变换完之后，我们使用感知机来训练一个分类器，如果我们让$\theta_0 = 0$(比如说我们让偏移量为0，而不是一定的偏移量) 那么我们会失去任何表达力吗？我们会让我们的模型的分类能力减弱吗？(would we lose any expressive power if we let $\theta_0 = 0$)
>
> 这个应该和之前的一样吧？

在经过4次迭代以后，感知机会找到一个分类器，这个分类器的系数是$\theta = (0, 0, 0, 0, 4, 0)$并且$\theta_0 = 0$, 那么相应的这个分类器的数学形式如下
$$
0 + 0x_1 + 0x_2+0x_1^2+4x_1x_2+0x_2^2 + 0 = 0
$$
化简一下其实就是$x_1x_2 = 0$, 也就是说$x_1$与$x_2$同号的话就被标注为正，如果异号则标注为负；

入下图所绘制的那样，灰色的区域被分类为负，白色的区域被分类为正；

> 一个问题: 一定要确保你理解为什么这个高维的超平面是一个分类器，这个高维度的超平面分类器是如何与这张图对应的？
>
> 因为我们使用的是高维度映射函数是$4x_1x_2 = 0$, 对于4个点，(1, 1), (-1, -1), (1, -1), (-1, 1)被划分成为了2类也就实现了划分；不过这里面的数学原理，我并不是很明白；

为了让我们的例子变得更有意思，我们在下面画了更多的图，下面就是在异或数据集上运行感知机算法的结果，但是这里的异或数据集中的4个点被分别放在了不同的位置，在感知机算法犯了65次错误之后，感知机算法得到了下面这个系数$\theta = (1, -1, -1, -5, 11, -5), \theta_0 = 1$, 这个感知机的数学形式如下：
$$
1*1 + (-1)*x_1 + (-1)*x_2 + (-5)*x_1^2 + (11)*x_1x_2 + (-5)x_2^2 = 0
$$
你可以自己用matplotlib绘制一下这个函数的图形，看看是不是和讲义上的图像一样；

> 我们之前说了，这个例子犯的错误多达65次，对这个版本的问题比用上一个版本的问题所花费的迭代次数要多很多，自己尝试用一下感知机的收敛理论来解释一下为什么这次的迭代次数比上次的迭代次数多那么多？
>
> 因为影响迭代次数的上界无非就两个因素，一个是$\mathcal{R}$, 也就是向量的长度，一个$\gamma$, 也就是所有点到分隔器的距离的上界，在这里，因为维数增加了，所以原来的$(1, 1)$被映射成了$(1, 1, 1, 1, 1, 1)$ 很显然$\mathcal{R}$变大了，那么相应的$(\frac {\mathcal{R}} \gamma)^2$ 也会增加；

下图中这个数据集是一个更难分的数据集，我们还是使用之前介绍的方法，结果花了200次迭代才吧结果找出来，要这个这个问题进行正确的分类，仅仅只是用2阶或者3阶的basis representation 还不够，至少要4阶才能正确的分出来；

> 有兴趣有时间的话，也可以将这个高阶的结果还原到原来的的二维平面，看一看在二维平面上的曲线是一个什么样子的；




#### 2. Hand-constructing features for real domains

在许多机器学习的应用中，我们被给予的数据来自不同的类型，这些数据都有各种各样的属性，比如说有的是数字类型的，有的是文字类型的，有的具有离散特征，而一个成功的机器学习模型的关键因素就是特征的选择以及特征被编码的的方式；

#### 2.1 Discrete features

对于离散的特征进行好的编码极其关键，你想要为你的机器学习系统找到潜在的规律创造良好的机会，尽管存在机器学习方法，这些机器学习方法有特殊的机制来处理离散的输入数据，但是在这门课上，我们会假设所有的输入的数据都是$\mathcal{R^d}$维中的向量。因此，我们必须弄清楚一些合理的策略来将离散的值转化为实数值；

我们首先列出一些编码的策略，然后用这些编码策略来应用到一些例子当中，我们假设我们在我们的原始数据中又一些特征，这些特征都是从k个离散值中取得的其中一个；

- **Numeric** 为这些值中的每一个值都赋一个数字，比如说 1.0/k, 2.0/k, …, 1.0. 我们可能想要去做一些更近一步的操作，比如在第8.3节中所描述的；这种编码方式只有当这些离散数值真的能够表明一定程度的数值量的时候才是合理的，也就是说，只有当这些数值的值是有意义的时候，这样编码才是合理的；
- **Thermometer code** 如果你的离散值有自然排序的，比如是是从1, …, k， 但是这种自然排序并不是自然映射到实数值，一种好的策略就是使用长度为k(也就是你的向量的长度)的二进制变量，这样一来，我们就讲离散的输入值 j $(0<j\le k)$ 转换为一个向量，在这个向量中，前j位的值为1，其他位的值为0；这不一定意味着关于输入数据的空间或者是数值量的信息， 但是可以传递出一些关于序号顺序的信息

- **Factored code** 如果你的离散值能够被很敏感的分解为两个部分(比如说一辆汽车的制造和建模)，那么最好的方式就是区别对待这两组分开的特征；然后对每一部分选择一个合适的方式进行单独编码；
- **One-hot code** 如果既没有明显的数值含义，没有明显的顺序含义，不能将数据明显的分成几类，那么最好的方法就是用一个长度为k的向量，在这个向量中，我们将离散的输入值$0<j\le k$ 转换成一个向量，这个向量中除了第$j$个元素的值为1之外，所有的其他的值都是0；
- **Binary code** 计算机科学家们往往会被诱惑使用Binary code, 因为binary code 能够让我们使用$\log k$ 长度的向量来代表k个值；事实上，这个想法非常糟糕！因为解码一个binary code 将会花费许多的计算量，而且除此之外，如果你采用这种方式来编码你的输入数据，那么你将会强迫你的系统去学习解码的算法，从而造成不必要的计算资源的浪费；

下面是一个例子，想象一下，你想要编码人的血型，而血型是从集合$\{A+, A-, B+, B-, AB+, AB-, O+, O-\}$里面得到的其中的一个，这个既没有明显的线性数值范围，也没有一定的顺序性，但是我么可以观察到存在一定的fractor,也就是说我们采用 factored code 是合理的；我们把这个数据集分成两个特征：$\{A, B, AB, O\}$ 和$\{+1, -1\}$, 而且，实际上，我们还可以继续将第一个group分成$\{A, not A\}, \{B, not B\}$, 因此下面是对于整个集合的两个可行的编码方法：

- 使用一个6维的向量，对于每一个factor使用两维，并且每个factor用one-hot进行编码；
- 使用一个3维的向量，每一个factor使用一维，对于每一个factor,如果它出现的话就标注为1，如果不出现就标注为-1(标注为-1比标注为0好)， 在这个例子里$AB+$会被编码为(1, 1, 1), 而$O-$会被编码为(-1, -1, -1)

> 为什么这里标注为-1比标注为0好呢？
>
> How would you encode A+ in both of these approaches? 
>
> - (1, 0, 0, 1, 1, 0)
>
> - (1, -1, 1)

#### 2.2 Text

如果要对一个文本(比如一条推特或者一个产品信息，甚至是这个讲义)进行编码，作为机器学习算法的输入，这个问题比较有意思，而且有点复杂，在这个课程的后半段，我们会研究序列化输入模型，在这个序列化输入模型中，我们并不需要将一个文本编码为一个固定长度的向量，我们将这个文本一个字一个字(甚至是一个字母一个字母的)喂入到一个hypothesis中；

对于基本应用来说，有一些更加简单的编码方式，这些编码方式非常的有效，其中的一个编码方式就是**bag of words(BOW)**模型，基本的想法就是让d作为在我们的词汇表中单词的数量(这个d要么是从训练集中计算出来的，要么从文本的其他部分或者是字典中计算出来的)。 之后，我们会造一个长度为d的0-1向量，如果单词j出现在了文本当中，那么第j个向量的值就为1，否则的话就为0；

#### 2.3 Numeric values

如果有一些特征已经被编码成了数值型的数据(比如说心率，证券的价格，距离等等；)那么通常情况下，你应该保持这些数值不变，他们是数值型数据，还让他们是数值型数据；不过这个只是通常的情况，有一个例外就是当你意识到在这个问题背景下其实是存在一个自然的断点的：比如说，要编码美国人的年龄，你可能会对于年龄在18岁(或者21岁)以上和以下的人做一个明确的区分，这种划分取决于你想要预测的结果是什么；你将它们分成不连续的区域，比如说按照前面说的根据年龄来划分，这样一来可以将非常年轻群体聚集到一起，当我们不希望年龄和一些生理特征只是线性(或者甚至只是单调的)关系的时候，我们可以采用独热码来编码，这样就比较有意义。

> 这个地方没有看明白，为什么在这个地方采用独热码就可以保证不是线性或者不是单调的呢？



如果你选择让一个本来是数值型的数据继续保留是数值型的，那么比较典型的做法就是进行归一化scale it，这样一来，你的数据的取值范围就只会在$[-1, +1]$之间波动，如果你不进行归一化处理的话，如果碰巧有一个特征比其他的特征大非常多，那么这样一来就会使算法花非常多的时间来找一个参数能够使得这些特征在同等的basis下面.

> 不太明白这里的 parameters that can put them on an equal basis. 不过如果不进行归一化处理的话，的却是会使得某些点的影响太重；不能让这个特征在同一个范围内做相同的比较，也就是说,有些特征的权重会占的更多一些；

因此，我们就可能会做如下的变换$\phi(x) = \frac {x - \bar x} {\sigma}$ 在这里$\bar x$ 是$x^{(i)}$的平均值，$\sigma$是$x^{(i)}$的标准差，特征值的结果将会是平均值为0，标准差为1，这种转换也被称为是标准化一个变量；

那么，当然了，你可能会应用一个高阶的基于多项式的变换来将变换一组或者多组数值型的特征；

> 思考题：
>
> 这个表达式的维度是12维，如果是Factored code的话，这种编码是合理的，并且这些数值是线性不可分的；



































































